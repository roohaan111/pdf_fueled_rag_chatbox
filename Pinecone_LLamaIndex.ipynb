{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pinecone and LLamaIndex incorporation",
   "id": "ce79ef1490ca323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:29:48.103331Z",
     "start_time": "2025-02-20T04:29:48.030906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore"
   ],
   "id": "d783455653ee0724",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:29:48.492770Z",
     "start_time": "2025-02-20T04:29:48.132113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pc = Pinecone(api_key=\"pcsk_3AJiBH_UPECMeGPwEbn4s5eamzJBPhgLZBXn2EyTimoEXBW1t9ZnBJ63PHgHvbHWH2TxPb\")\n",
    "\n",
    "index_name = \"test\"\n",
    "\n",
    "# Check if index exists, if not, create it\n",
    "existing_indexes = [index_info.name for index_info in pc.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "# Connect to Pinecone index\n",
    "pinecone_index = pc.Index(index_name)\n"
   ],
   "id": "6ed3e1c4ebbfedd6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:31:45.979149Z",
     "start_time": "2025-02-20T04:31:41.522419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up embedding model using Hugging Face\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Set up LlamaIndex's LLM using Ollama\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Load PDFs using LlamaIndex's SimpleDirectoryReader\n",
    "pdf_folder = \"/Users/avilochab/Desktop/Job: Work/Chatbox/files\"\n",
    "documents = SimpleDirectoryReader(pdf_folder, required_exts=[\".pdf\"]).load_data()"
   ],
   "id": "204daad3407667c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:31:49.409127Z",
     "start_time": "2025-02-20T04:31:48.474191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure LlamaIndex settings\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "# Use Pinecone as the Vector Store in LlamaIndex\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "index = VectorStoreIndex.from_documents(documents, vector_store=vector_store)\n"
   ],
   "id": "1ec3cc09020b8e2c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:31:53.996625Z",
     "start_time": "2025-02-20T04:31:49.411749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve relevant context\n",
    "query_engine = index.as_query_engine(response_mode=\"no_text\", use_source_nodes=True)\n",
    "query = \"what is the intent behind a scammer\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Extract context from retrieved source nodes\n",
    "extracted_texts = [node.text for node in response.source_nodes]\n",
    "\n",
    "# Format prompt in your required style\n",
    "context = \" \".join(extracted_texts)\n",
    "prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "\n",
    "# Send formatted prompt to the LLM\n",
    "final_response = llm.complete(prompt)\n",
    "\n",
    "# Print Answer and Citations\n",
    "print(\"\\nAnswer:\\n\", final_response)\n",
    "print(\"\\nCitations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- {node.metadata.get('file_path', 'Unknown Source')}\")"
   ],
   "id": "3aa821ee1a6d9395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " The intent behind a scammer in this context is to trick a victim into divulging their sensitive information (e.g., username, password, 2-factor authentication code) and then using that information to transfer money out of the victim's account. The ultimate goal is to steal money from the victim.\n",
      "\n",
      "Citations:\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/scammer-agent.pdf\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/scammer-agent.pdf\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:31:58.880710Z",
     "start_time": "2025-02-20T04:31:54.015389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve relevant context\n",
    "query_engine = index.as_query_engine(response_mode=\"no_text\", use_source_nodes=True)\n",
    "query = \"Explain llm in 100 words\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Extract context from retrieved source nodes\n",
    "extracted_texts = [node.text for node in response.source_nodes]\n",
    "\n",
    "# Format prompt in your required style\n",
    "context = \" \".join(extracted_texts)\n",
    "prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "\n",
    "# Send formatted prompt to the LLM\n",
    "final_response = llm.complete(prompt)\n",
    "\n",
    "# Print Answer and Citations\n",
    "print(\"\\nAnswer:\\n\", final_response)\n",
    "print(\"\\nCitations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- {node.metadata.get('file_path', 'Unknown Source')}\")"
   ],
   "id": "803764e485e7ec4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " Here is an explanation of LLM in 100 words:\n",
      "\n",
      "LLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on vast amounts of text data to generate human-like responses. LLMs are designed to understand and respond to natural language inputs, making them useful for tasks such as language translation, text summarization, and conversation generation. The models' ability to learn patterns in language allows them to adapt to new tasks and domains, but they can also struggle with nuances and context-dependent information. Researchers continue to improve LLMs through techniques like prompt engineering, adaptation strategies, and self-reflection to enhance their performance.\n",
      "\n",
      "Citations:\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/LLMs.pdf\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/aviationGPT.pdf\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:32:03.707963Z",
     "start_time": "2025-02-20T04:31:58.897769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve relevant context\n",
    "query_engine = index.as_query_engine(response_mode=\"no_text\", use_source_nodes=True)\n",
    "query = \"Explain special forces in 100 words\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Extract context from retrieved source nodes\n",
    "extracted_texts = [node.text for node in response.source_nodes]\n",
    "\n",
    "# Format prompt in your required style\n",
    "context = \" \".join(extracted_texts)\n",
    "prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "\n",
    "# Send formatted prompt to the LLM\n",
    "final_response = llm.complete(prompt)\n",
    "\n",
    "# Print Answer and Citations\n",
    "print(\"\\nAnswer:\\n\", final_response)\n",
    "print(\"\\nCitations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- {node.metadata.get('file_path', 'Unknown Source')}\")"
   ],
   "id": "21bfd0ffe890f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " Here is a 100-word explanation of special forces:\n",
      "\n",
      "Special forces are military units that conduct unique operations requiring specialized skills, equipment, and training. These operations often take place in hostile or sensitive environments, with limited visibility and high risk. Special Operations Forces (SOF) are designed to support conventional military operations by conducting clandestine, low-visibility, and time-sensitive missions. They work closely with indigenous forces and require regional expertise. SOF units are organized, trained, and equipped to conduct special operations, including counterterrorism, direct action, and unconventional warfare. Their primary goal is to disrupt or defeat enemy organizations in support of national security objectives.\n",
      "\n",
      "Citations:\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/SOF.pdf\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/SOF.pdf\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T04:32:08.033036Z",
     "start_time": "2025-02-20T04:32:03.739394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve relevant context\n",
    "query_engine = index.as_query_engine(response_mode=\"no_text\", use_source_nodes=True)\n",
    "query = \"explain aviationGPT in 100 words\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Extract context from retrieved source nodes\n",
    "extracted_texts = [node.text for node in response.source_nodes]\n",
    "\n",
    "# Format prompt in your required style\n",
    "context = \" \".join(extracted_texts)\n",
    "prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "\n",
    "# Send formatted prompt to the LLM\n",
    "final_response = llm.complete(prompt)\n",
    "\n",
    "# Print Answer and Citations\n",
    "print(\"\\nAnswer:\\n\", final_response)\n",
    "print(\"\\nCitations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- {node.metadata.get('file_path', 'Unknown Source')}\")"
   ],
   "id": "62ba0e88fa272b4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " AviationGPT is a cutting-edge language model designed to tackle complex Natural Language Processing (NLP) problems in the aviation domain. It offers several advantages, including versatility, accurate and contextually relevant responses, improved performance over rule-based methods, reduced model development and maintenance costs, and enhanced utilization of text data. By leveraging large-scale aviation domain data and advanced modeling techniques, AviationGPT aims to enhance the efficiency and safety of NAS operations, such as discovering hidden patterns in maintenance logs.\n",
      "\n",
      "Citations:\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/aviationGPT.pdf\n",
      "- /Users/avilochab/Desktop/Job: Work/Chatbox/files/aviationGPT.pdf\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cceab0fb5627f712"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
